{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['publish_date'] = pd.to_datetime(data['publish_date'].astype(str), format = '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['publish_date'] = pd.DatetimeIndex(data['publish_date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0          2003  aba decides against community broadcasting lic...\n",
      "1          2003     act fire witnesses must be aware of defamation\n",
      "2          2003     a g calls for infrastructure protection summit\n",
      "3          2003           air nz staff in aust strike for pay rise\n",
      "4          2003      air nz strike to affect australian travellers\n"
     ]
    }
   ],
   "source": [
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['headline_text'] = data.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop.extends(['interview', 'weather', 'abc', 'australia', 'australian'])\n",
    "data['headline_text'] = data['headline_text'].apply(lambda x: [word for word in x if word not in (stop)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0          2003   [aba, decides, community, broadcasting, licence]\n",
      "1          2003    [act, fire, witnesses, must, aware, defamation]\n",
      "2          2003     [g, calls, infrastructure, protection, summit]\n",
      "3          2003          [air, nz, staff, aust, strike, pay, rise]\n",
      "4          2003  [air, nz, strike, affect, australian, travellers]\n"
     ]
    }
   ],
   "source": [
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['headline_text'] = data['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0          2003       [aba, decide, community, broadcast, licence]\n",
      "1          2003      [act, fire, witness, must, aware, defamation]\n",
      "2          2003      [g, call, infrastructure, protection, summit]\n",
      "3          2003          [air, nz, staff, aust, strike, pay, rise]\n",
      "4          2003  [air, nz, strike, affect, australian, travellers]\n"
     ]
    }
   ],
   "source": [
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_doc = data['headline_text'].apply(lambda x: [word for word in x if len(word) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [aba, decide, community, broadcast, licence]\n",
      "1    [act, fire, witness, must, aware, defamation]\n",
      "2       [call, infrastructure, protection, summit]\n",
      "3            [air, staff, aust, strike, pay, rise]\n",
      "4    [air, strike, affect, australian, travellers]\n",
      "Name: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_doc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "data['headline_text'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     aba decide community broadcast licence\n",
       "1     act fire witness must aware defamation\n",
       "2      call infrastructure protection summit\n",
       "3             air staff aust strike pay rise\n",
       "4    air strike affect australian travellers\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['headline_text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    globals()['trend{}'.format(i+2003)] = temp.loc[temp.publish_date == i+2003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_list = [trend2003, trend2004, trend2005, trend2006, trend2007, trend2008, trend2009, trend2010, trend2011, trend2012, trend2013, trend2014, trend2015, trend2016, trend2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(year, components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(str(year)+\" Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003 Topic 1: [('charge', 361.87), ('warn', 277.09), ('cup', 249.3), ('world', 241.23), ('set', 241.0)]\n",
      "2003 Topic 2: [('police', 249.95), ('home', 203.29), ('open', 199.97), ('drug', 188.48), ('probe', 184.98)]\n",
      "2003 Topic 3: [('police', 445.67), ('seek', 262.58), ('qld', 247.23), ('say', 243.67), ('group', 240.08)]\n",
      "2003 Topic 4: [('face', 352.53), ('court', 254.1), ('fund', 233.28), ('health', 231.18), ('concern', 223.18)]\n",
      "2003 Topic 5: [('kill', 415.53), ('claim', 315.65), ('report', 295.58), ('attack', 262.86), ('miss', 237.02)]\n",
      "2004 Topic 1: [('plan', 652.15), ('govt', 394.64), ('council', 382.67), ('urge', 344.86), ('water', 300.03)]\n",
      "2004 Topic 2: [('fear', 222.92), ('strike', 217.86), ('child', 200.69), ('return', 187.58), ('push', 183.6)]\n",
      "2004 Topic 3: [('police', 380.73), ('miss', 268.99), ('continue', 268.74), ('drug', 248.65), ('lead', 238.85)]\n",
      "2004 Topic 4: [('man', 541.35), ('boost', 339.13), ('charge', 317.37), ('court', 258.09), ('minister', 248.5)]\n",
      "2004 Topic 5: [('labor', 254.11), ('ban', 251.13), ('test', 243.36), ('hit', 236.85), ('change', 230.07)]\n",
      "2005 Topic 1: [('kill', 420.17), ('police', 395.83), ('man', 376.74), ('court', 346.86), ('crash', 340.39)]\n",
      "2005 Topic 2: [('govt', 681.7), ('urge', 510.02), ('boost', 277.88), ('plan', 235.5), ('concern', 234.55)]\n",
      "2005 Topic 3: [('service', 271.97), ('police', 268.19), ('house', 225.61), ('farmers', 209.58), ('push', 198.0)]\n",
      "2005 Topic 4: [('council', 324.8), ('water', 310.58), ('iraq', 248.45), ('fund', 244.44), ('consider', 237.17)]\n",
      "2005 Topic 5: [('claim', 318.77), ('group', 317.74), ('health', 309.09), ('report', 288.84), ('miss', 262.39)]\n",
      "2006 Topic 1: [('council', 406.08), ('urge', 336.41), ('seek', 291.37), ('govt', 284.95), ('change', 274.9)]\n",
      "2006 Topic 2: [('plan', 350.96), ('crash', 313.77), ('water', 256.63), ('govt', 255.33), ('health', 227.8)]\n",
      "2006 Topic 3: [('fund', 335.67), ('group', 239.11), ('win', 217.56), ('drought', 214.44), ('aust', 204.89)]\n",
      "2006 Topic 4: [('closer', 463.68), ('charge', 362.38), ('face', 294.93), ('police', 287.16), ('death', 242.41)]\n",
      "2006 Topic 5: [('man', 249.99), ('help', 243.42), ('miss', 232.2), ('house', 214.21), ('labor', 211.12)]\n",
      "2007 Topic 1: [('nsw', 326.3), ('car', 243.77), ('power', 230.1), ('set', 221.94), ('open', 215.34)]\n",
      "2007 Topic 2: [('water', 462.66), ('closer', 419.7), ('plan', 334.38), ('change', 317.45), ('govt', 293.14)]\n",
      "2007 Topic 3: [('police', 703.35), ('man', 652.23), ('charge', 499.56), ('court', 414.44), ('warn', 328.53)]\n",
      "2007 Topic 4: [('miss', 305.94), ('group', 259.6), ('boost', 244.84), ('aust', 244.52), ('help', 238.07)]\n",
      "2007 Topic 5: [('claim', 243.42), ('seek', 236.87), ('crash', 235.15), ('make', 229.21), ('dead', 202.83)]\n",
      "2008 Topic 1: [('cut', 345.97), ('change', 272.91), ('arrest', 252.62), ('open', 230.57), ('push', 215.89)]\n",
      "2008 Topic 2: [('plan', 506.98), ('health', 272.5), ('drug', 252.58), ('govt', 250.08), ('fund', 233.3)]\n",
      "2008 Topic 3: [('man', 361.28), ('face', 335.3), ('miss', 298.5), ('jail', 297.54), ('power', 239.33)]\n",
      "2008 Topic 4: [('qld', 421.75), ('warn', 299.19), ('nsw', 258.0), ('coast', 225.3), ('gold', 223.96)]\n",
      "2008 Topic 5: [('kill', 492.86), ('police', 344.92), ('man', 342.47), ('school', 273.65), ('crash', 273.28)]\n",
      "2009 Topic 1: [('home', 299.45), ('coast', 224.96), ('cut', 218.94), ('australia', 212.83), ('power', 189.2)]\n",
      "2009 Topic 2: [('accuse', 307.94), ('win', 300.13), ('flu', 279.02), ('swine', 252.54), ('police', 210.41)]\n",
      "2009 Topic 3: [('plan', 437.39), ('court', 377.55), ('crash', 373.83), ('face', 320.37), ('man', 310.25)]\n",
      "2009 Topic 4: [('charge', 367.27), ('man', 340.39), ('warn', 319.11), ('murder', 301.47), ('water', 244.92)]\n",
      "2009 Topic 5: [('interview', 765.74), ('say', 297.53), ('report', 288.4), ('boost', 252.15), ('blaze', 206.42)]\n",
      "2010 Topic 1: [('warn', 301.82), ('jail', 297.11), ('house', 292.7), ('health', 261.81), ('green', 251.11)]\n",
      "2010 Topic 2: [('council', 333.4), ('face', 313.29), ('attack', 296.14), ('police', 286.85), ('man', 228.1)]\n",
      "2010 Topic 3: [('interview', 651.39), ('water', 315.47), ('day', 253.2), ('flood', 228.93), ('open', 212.06)]\n",
      "2010 Topic 4: [('crash', 363.7), ('man', 286.83), ('miss', 276.11), ('rise', 235.65), ('woman', 234.92)]\n",
      "2010 Topic 5: [('win', 285.71), ('cup', 192.93), ('tax', 190.83), ('case', 174.19), ('hit', 173.71)]\n",
      "2011 Topic 1: [('man', 320.23), ('jail', 264.72), ('sport', 264.31), ('weather', 260.17), ('charge', 259.0)]\n",
      "2011 Topic 2: [('interview', 553.68), ('say', 207.63), ('media', 204.78), ('boost', 189.28), ('strike', 188.65)]\n",
      "2011 Topic 3: [('abc', 648.0), ('water', 277.53), ('open', 271.29), ('business', 228.67), ('hospital', 208.07)]\n",
      "2011 Topic 4: [('crash', 348.6), ('market', 300.55), ('fear', 270.38), ('australia', 241.46), ('court', 228.77)]\n",
      "2011 Topic 5: [('council', 318.66), ('plan', 304.72), ('news', 295.4), ('report', 288.59), ('murder', 262.94)]\n",
      "2012 Topic 1: [('abc', 927.88), ('interview', 584.95), ('market', 578.77), ('weather', 489.63), ('news', 434.52)]\n",
      "2012 Topic 2: [('say', 443.47), ('report', 416.13), ('cut', 384.28), ('job', 318.48), ('government', 300.56)]\n",
      "2012 Topic 3: [('crash', 343.04), ('australian', 332.89), ('nsw', 271.89), ('price', 266.12), ('green', 242.83)]\n",
      "2012 Topic 4: [('court', 353.11), ('murder', 312.36), ('plan', 298.8), ('house', 278.97), ('entertainment', 265.43)]\n",
      "2012 Topic 5: [('police', 700.78), ('man', 348.47), ('jail', 292.06), ('water', 270.67), ('hit', 264.13)]\n",
      "2013 Topic 1: [('rural', 604.22), ('country', 449.35), ('market', 391.97), ('crash', 335.29), ('nrn', 315.66)]\n",
      "2013 Topic 2: [('council', 412.47), ('plan', 316.96), ('kill', 271.53), ('hit', 261.29), ('farm', 251.97)]\n",
      "2013 Topic 3: [('man', 493.61), ('charge', 481.3), ('police', 440.36), ('hour', 420.36), ('grandstand', 251.68)]\n",
      "2013 Topic 4: [('interview', 532.15), ('2013', 438.96), ('government', 349.55), ('nsw', 341.75), ('australian', 273.82)]\n",
      "2013 Topic 5: [('court', 446.37), ('day', 328.46), ('murder', 319.74), ('accuse', 255.25), ('ash', 248.49)]\n",
      "2014 Topic 1: [('nrn', 276.79), ('health', 262.36), ('live', 240.09), ('job', 227.29), ('die', 222.51)]\n",
      "2014 Topic 2: [('government', 331.31), ('world', 296.32), ('cup', 276.13), ('new', 217.21), ('school', 215.34)]\n",
      "2014 Topic 3: [('2014', 639.46), ('country', 621.01), ('rural', 618.63), ('interview', 596.15), ('hour', 591.13)]\n",
      "2014 Topic 4: [('market', 291.45), ('report', 285.7), ('council', 274.29), ('open', 264.95), ('cut', 247.06)]\n",
      "2014 Topic 5: [('kill', 244.82), ('win', 230.19), ('queensland', 228.86), ('state', 213.66), ('jail', 211.0)]\n",
      "2015 Topic 1: [('rural', 449.9), ('national', 267.61), ('attack', 238.26), ('canberra', 237.54), ('plan', 228.36)]\n",
      "2015 Topic 2: [('man', 359.85), ('world', 320.01), ('police', 315.18), ('day', 279.08), ('perth', 245.75)]\n",
      "2015 Topic 3: [('help', 210.35), ('china', 209.03), ('say', 190.01), ('concern', 185.46), ('woman', 180.31)]\n",
      "2015 Topic 4: [('live', 245.37), ('say', 239.75), ('farm', 186.77), ('league', 186.29), ('job', 184.36)]\n",
      "2015 Topic 5: [('2015', 665.64), ('country', 590.92), ('hour', 548.89), ('australian', 413.48), ('court', 311.59)]\n",
      "2016 Topic 1: [('man', 458.09), ('police', 295.6), ('charge', 283.91), ('court', 243.22), ('sydney', 201.18)]\n",
      "2016 Topic 2: [('queensland', 237.98), ('market', 198.71), ('australian', 175.47), ('share', 149.62), ('tasmania', 147.49)]\n",
      "2016 Topic 3: [('trump', 310.61), ('rio', 184.24), ('turnbull', 146.39), ('donald', 135.6), ('drum', 130.71)]\n",
      "2016 Topic 4: [('election', 322.5), ('help', 166.98), ('hospital', 154.38), ('tasmanian', 143.35), ('flood', 139.27)]\n",
      "2016 Topic 5: [('australia', 393.31), ('change', 186.48), ('open', 148.44), ('people', 144.5), ('day', 144.18)]\n",
      "2017 Topic 1: [('turnbull', 135.77), ('power', 126.2), ('market', 125.36), ('interview', 114.82), ('health', 112.83)]\n",
      "2017 Topic 2: [('north', 195.85), ('help', 131.53), ('korea', 130.22), ('drum', 126.32), ('time', 112.28)]\n",
      "2017 Topic 3: [('police', 163.19), ('government', 143.34), ('face', 138.3), ('south', 135.88), ('shoot', 131.87)]\n",
      "2017 Topic 4: [('house', 171.48), ('murder', 148.03), ('report', 147.92), ('death', 147.65), ('jail', 110.5)]\n",
      "2017 Topic 5: [('trump', 262.78), ('sex', 199.65), ('day', 174.03), ('court', 154.63), ('donald', 145.47)]\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(trend_list):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', \n",
    "    max_features= len(v)) # 상위 1,000개의 단어를 보존 \n",
    "    X = vectorizer.fit_transform(v['headline_text'])\n",
    "     # TF-IDF 행렬의 크기 확인\n",
    "#     print(\"trend\"+str(i+2003)+\": \",X.shape)\n",
    "    lda_model=LatentDirichletAllocation(n_components=5,learning_method='online')\n",
    "    lda_top=lda_model.fit_transform(X)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    get_topics(i+2003, lda_model.components_,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model=LatentDirichletAllocation(n_components=5,learning_method='online')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_top=lda_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('police', 791.27), ('new', 631.75), ('plan', 568.03), ('man', 534.98), ('govt', 490.47)]\n"
     ]
    }
   ],
   "source": [
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(lda_model.components_,terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
