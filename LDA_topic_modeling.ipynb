{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\past\\types\\oldstr.py:33: DeprecationWarning: invalid escape sequence \\d\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines =False)\n",
    "data['publish_date'] = pd.to_datetime(data['publish_date'].astype(str), format = '%Y%m%d')\n",
    "data['publish_date'] = pd.DatetimeIndex(data['publish_date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "        \n",
    "        \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data['headline_text']))\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_nostops = pd.Series(data_words_nostops)\n",
    "data_lemmatized = data_words_nostops.apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickle_data/data_lemmatized.pickle', 'wb') as f:\n",
    "    pickle.dump(data_lemmatized, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_lemmatized.pickle', 'rb') as f:\n",
    "    data_lemmatized = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(['interview', 'weather', 'abc', 'australia', 'australian','queensland', 'australias','sydney', 'nz', 'plan','get','back','new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(['urge','group','council','say','report','two','qld','call','nsw','may','take','seek','police','make','tas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(['meet','win','talk', 'govt','find','man','set','wa','continue','miss','three','one','two','welcome','fire','year','sa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(['melbourne','first','second','third','four','fourth','face','adelaide','government','govt','news','years','canberra','brisbane','offer',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = remove_stopwords(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data\n",
    "temp['lemmatize'] = data_lemmatized\n",
    "for i in range(15):\n",
    "    globals()['trend{}'.format(i+2003)] = temp.loc[temp.publish_date == i+2003]\n",
    "\n",
    "trend_list = [trend2003, trend2004, trend2005, trend2006, trend2007, trend2008, trend2009, trend2010, trend2011, trend2012, trend2013, trend2014, trend2015, trend2016, trend2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, year in enumerate(trend_list):    \n",
    "    num_topics = 20\n",
    "    passes = 10\n",
    "    chunksize = 8000\n",
    "    lemmatized = list(year['lemmatize'])\n",
    "    id2word = corpora.Dictionary(lemmatized)\n",
    "#     id2word.filter_extremes(no_below=5, no_above=0.3, keep_n=10000)\n",
    "    texts = lemmatized\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=chunksize,\n",
    "                                           passes=passes,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    \n",
    "    with open(\"./pickle_data_topic_20/result_\"+str(i+2003)+\".pickle\", 'wb') as f:\n",
    "        pickle.dump(lda_model.show_topics(num_topics=num_topics, num_words=20, log=False, formatted=True), f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    pyLDAvis.enable_notebook()\n",
    "    prepared_data  = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    pyLDAvis.save_html(prepared_data, './result_topic_20/LDA_result'+str(i+2003)+'.html')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
